# TM-Speech - PyTorch Implementation

This is a PyTorch implementation of the text-to-speech system TM-Speech. 
This project is based on [**FastSpeech 2: Fast and High-Quality End-to-End Text to Speech**](https://arxiv.org/abs/2006.04558v1). 

# Audio Samples
Audio samples generated by this implementation can be found here(Upload soon). 

# Quickstart
## Pre-requisites
You can install the Python dependencies with
```
pip install -r requirements.txt
```
## Prepare Dataset
### Datasets

The supported datasets are

- [LJSpeech](https://keithito.com/LJ-Speech-Dataset/): a single-speaker English dataset consists of 13100 short audio clips of a female speaker reading passages from 7 non-fiction books, approximately 24 hours in total.
- [AISHELL-3](http://www.aishelltech.com/aishell_3): a Mandarin TTS dataset with 218 male and female speakers, roughly 85 hours in total.

We take LJSpeech as an example hereafter.

### Preprocessing
 
First, run 
```
python3 prepare_align.py config/LJSpeech/preprocess.yaml
```
for some preparations.

As described in the paper, [Montreal Forced Aligner](https://montreal-forced-aligner.readthedocs.io/en/latest/) (MFA) is used to obtain the alignments between the utterances and the phoneme sequences.
Alignments of the supported datasets are provided [here](https://drive.google.com/drive/folders/1DBRkALpPd6FL9gjHMmMEdHODmkgNIIK4?usp=sharing).
You have to unzip the files in ``preprocessed_data/LJSpeech/TextGrid/``.

After that, run the preprocessing script by
```
python3 preprocess.py config/LJSpeech/preprocess.yaml
```

Alternately, you can align the corpus by yourself. 
Download the official MFA package and run
```
./montreal-forced-aligner/bin/mfa_align raw_data/LJSpeech/ lexicon/librispeech-lexicon.txt english preprocessed_data/LJSpeech
```
or
```
./montreal-forced-aligner/bin/mfa_train_and_align raw_data/LJSpeech/ lexicon/librispeech-lexicon.txt preprocessed_data/LJSpeech
```

to align the corpus and then run the preprocessing script.
```
python3 preprocess.py config/LJSpeech/preprocess.yaml
```

# Training

Train your model with
```
python3 train.py -p config/LJSpeech/preprocess.yaml -m config/LJSpeech/model.yaml
```
# Inference

You have to download the [pretrained models](https://drive.google.com/drive/folders/1BHmpAgYThXJh0gygzDnEpBzEZyVKzKdI?usp=sharing) and put them in ``output/ckpt/LJSpeech/`` or  ``output/ckpt/AISHELL3``.

For English single-speaker TTS, run
```
python3 synthesize.py --text "YOUR_DESIRED_TEXT" --restore_step 60000 --mode single -p config/LJSpeech/preprocess.yaml -m config/LJSpeech/model.yaml
```

For Mandarin multi-speaker TTS, try
```
python3 synthesize.py --text "大家好" --speaker_id SPEAKER_ID --restore_step 90000 --mode single -p config/AISHELL3/preprocess.yaml -m config/AISHELL3/model.yaml
```

The generated utterances will be put in ``output/result/``.

## Controllability
The pitch/volume/speaking rate of the synthesized utterances can be controlled by specifying the desired pitch/energy/duration ratios.
For example, one can increase the speaking rate by 20 % and decrease the volume by 20 % by

```
python3 synthesize.py --text "YOUR_DESIRED_TEXT" --restore_step 900000 --mode single -p config/LJSpeech/preprocess.yaml -m config/LJSpeech/model.yaml --duration_control 0.8 --energy_control 0.8
```
# TensorBoard

Use
```
tensorboard --logdir output/log/LJSpeech
```

to serve TensorBoard on your localhost.
We compared FastSpeech2, Mamba-Transfomer and our TM-Speech based on encoder-decoder architecture. The following figure is the visualization loss in tensorboard during training.

![image](https://github.com/Apolarity886/TM-Speech/blob/be2679d44618b36d9f090d20c48ec1530a980f8e/img/loss%20picture.png))
![image](https://github.com/Apolarity886/TM-Speech/blob/be2679d44618b36d9f090d20c48ec1530a980f8e/img/mel%20picture.png)
![image](https://github.com/Apolarity886/TM-Speech/blob/2312d943028fc195ace1e4d36fd3b8467a0531c6/img/audio%20picture.png)

# Implementation Issues

- Following [xcmyz's implementation](https://github.com/xcmyz/FastSpeech), I use an additional Tacotron-2-styled Post-Net after the decoder, which is not used in the original FastSpeech 2.
- Gradient clipping is used in the training.


# References
- [FastSpeech 2: Fast and High-Quality End-to-End Text to Speech](https://arxiv.org/abs/2006.04558), Y. Ren, *et al*.

# Citation
```

```
